{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys9Rj0sVqrl8"
      },
      "source": [
        "# **Audio Embeddings With Tabular Classification Model**\n",
        "\n",
        "In this example, we will build an audio classification model using `PyTorch` and `Wav2Vec2`, a pretrained model for processing audio data. This guide will walk you through each step of the process, including setting up the environment, loading and preprocessing data, defining and training a model, and evaluating its performance.\n",
        "\n",
        "## Tips\n",
        "For best performance, ensure that the runtime is set to use a GPU (`Runtime > Change runtime type > T4 GPU`).\n",
        "\n",
        "## Help & Questions\n",
        "\n",
        "If you have any questions, please reachout on our [Discord](https://discord.gg/dncQwFdN9m).\n",
        "\n",
        "You can also use our [documenation](https://docs.modlee.ai/README.html) as a reference for using our package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMRtYCjiq43e"
      },
      "source": [
        "\n",
        "## Step 1: Environment Setup\n",
        "\n",
        "First, we need to make sure that we have the necessary packages installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWn6dELrq-ye"
      },
      "source": [
        "This command mounts your Google Drive to the Colab environment. You'll be able to access files in your Drive through the /content/drive directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HqUbglYsCEh"
      },
      "source": [
        "## Step 2: Downloading the Dataset from Kaggle\n",
        "\n",
        "For this example, we will manually download the diabetes dataset from Kaggle and upload it to your Google Colab environment.\n",
        "\n",
        "1. Visit the [Human words Audio Classification](https://www.kaggle.com/datasets/warcoder/cats-vs-dogs-vs-birds-audio-classification?resource=download) dataset on Kaggle.\n",
        "2. Click the \"Download\" button to save the dataset to your local machine.\n",
        "3. Upload the `Animal` directory to you Google Drive.\n",
        "4. In your Colab notebook, click on the file icon on the left side and look for the `Animal` directory in your mounted Google Drive.\n",
        "\n",
        "This section ensures that the dataset is ready for use in the subsequent steps. Copy the path to the dataset from your Colab environment. It will look something like this, `/content/drive/MyDrive/Animals`, and you can use this in your data processing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7ZFXHdtXsL"
      },
      "source": [
        "## Step 3:  Importing Required Libraries\n",
        "Now, import the libraries needed for the rest of the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install modlee torch torchvision torchaudio pytorch-lightning torchtext==0.18.0 soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_rOHivUTtynd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/16/t5sssn69325c9k5wcws7c5fw0000gn/T/ipykernel_73368/144408680.py:11: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"sox_io\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torchaudio\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import Wav2Vec2Model\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import os\n",
        "import modlee\n",
        "import lightning.pytorch as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "torchaudio.set_audio_backend(\"sox_io\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UHfZnp64UTX"
      },
      "source": [
        "Now we will set our Modlee API key and initialize the Modlee package.\n",
        "Make sure that you have a Modlee account and an API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
        "Replace `replace-with-your-api-key` with your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sQcNrLeB4VpE"
      },
      "outputs": [],
      "source": [
        "# Set the API key to an environment variable,\n",
        "# to simulate setting this in your shell profile\n",
        "os.environ['MODLEE_API_KEY'] = \"OktSzjtS27JkuFiqpuzzyZCORw88Cz0P\"\n",
        "modlee.init(api_key=os.environ['MODLEE_API_KEY'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ZH52NVt2Yc"
      },
      "source": [
        "## Step 4: Loading the Pretrained Wav2Vec2 Model\n",
        "This snippet loads the Wav2Vec2 model. Wav2Vec2 is a model designed for speech processing. We'll use it to convert audio into embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280,
          "referenced_widgets": [
            "efaeb08642fd46a8a6b79a8842a3e7af",
            "6d533c8d13414a38a27a5c398536404a",
            "9a887d9bdc6541c1a4274ee573bb8d99",
            "83e47eac98f6488aaf72d7d7eb34f26f",
            "5ae2205d65a148bc96f803393f95ae18",
            "246b015e6d2f4dda8d33769648149aba",
            "bc067f3b8f22430e9c12a0b71e4951c6",
            "51109b3d3f1e4e81a3841696614d08f9",
            "1763c110771646e2a469cbf442c7b672",
            "3f33159c1217480786ba4c275f4be9c3",
            "848e0109a6cb4a019336537873ec6fca",
            "47c3c346b9654f93b107dba87fbd6b5e",
            "4a9fa9435dea424b912cfc451df59f9e",
            "3bfa78db43e44802a478ad1fc8c796ee",
            "6bb745aa8fcc4c968ca24eb537e8a5c1",
            "08b37d0097fa4853812666fee8e13a43",
            "daa191fed4144211af076236886f8dff",
            "da64e8aa5619422e9b267faeae4f21cd",
            "c80d636fccfb4283855af9724843334a",
            "ca5c2d240740426882707a5045d12a1b",
            "aa730613929e4934a283a29b344baeed",
            "7c2744737c984e2b8eeea47e814aad35"
          ]
        },
        "id": "kKnuudJQuOOU",
        "outputId": "05edfa37-43d2-4315-ced5-de782d4b60e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mansiagrawal/Documents/modlee_pypi/venv/lib/python3.12/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Set device to GPU if available, otherwise use CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained Wav2Vec2 model and move it to the specified device.\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_3PhrzguTIu"
      },
      "source": [
        "## Step 5: Extracting Wav2Vec2 Embeddings\n",
        "This function converts raw audio waveforms into embeddings using the Wav2Vec2 model. The embeddings are used as features for our classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bpyfkvxbwDb0"
      },
      "outputs": [],
      "source": [
        "def get_wav2vec_embeddings(waveforms):\n",
        "    with torch.no_grad():  # Turn off gradients to save memory during inference\n",
        "        # Convert waveforms to a tensor and move it to the chosen device\n",
        "        inputs = torch.tensor(waveforms).to(device)\n",
        "        # Get embeddings from the Wav2Vec2 model\n",
        "        embeddings = wav2vec(inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_KGZBbwFO4"
      },
      "source": [
        "## Step 6: Creating a Custom Dataset Class\n",
        "The `AudioDataset` class handles loading and preprocessing of audio files. This includes padding or truncating audio samples to a fixed length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CharPFSPwRYF"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(TensorDataset):\n",
        "    def __init__(self, audio_paths, labels, target_length=16000):\n",
        "        self.audio_paths = audio_paths  # List of paths to audio files\n",
        "        self.labels = labels  # List of labels corresponding to audio files\n",
        "        self.target_length = target_length  # Desired length for audio clips\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_paths)  # Number of items in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_paths[idx]  # Get the path of the audio file\n",
        "        label = self.labels[idx]  # Get the label for the audio file\n",
        "        waveform, sample_rate = torchaudio.load(audio_path, normalize=True)  # Load and normalize the audio\n",
        "        waveform = waveform.mean(dim=0)  # Convert to mono by averaging channels\n",
        "\n",
        "        # Pad or truncate the waveform to the target length\n",
        "        if waveform.size(0) < self.target_length:\n",
        "            waveform = torch.cat([waveform, torch.zeros(self.target_length - waveform.size(0))])\n",
        "        else:\n",
        "            waveform = waveform[:self.target_length]\n",
        "\n",
        "        return waveform, label  # Return the processed waveform and its label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAK6bfOfwSCl"
      },
      "source": [
        "## Step 7: Loading and Preprocessing the Dataset\n",
        "This function loads audio files and their corresponding labels from a directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IXN2QIS6wZtN"
      },
      "outputs": [],
      "source": [
        "def load_dataset(data_dir):\n",
        "    audio_paths = []  # List to store paths to audio files\n",
        "    labels = []  # List to store labels corresponding to each audio file\n",
        "\n",
        "    # Loop through each subdirectory in the data directory\n",
        "    for label_dir in os.listdir(data_dir):\n",
        "        label_dir_path = os.path.join(data_dir, label_dir)\n",
        "        if os.path.isdir(label_dir_path):  # Check if it's a directory\n",
        "            # Loop through each file in the directory\n",
        "            for file_name in os.listdir(label_dir_path):\n",
        "                if file_name.endswith('.wav'):  # Check if the file is a .wav file\n",
        "                    audio_paths.append(os.path.join(label_dir_path, file_name))  # Add file path to list\n",
        "                    labels.append(label_dir)  # Add label (directory name) to list\n",
        "\n",
        "    return audio_paths, labels  # Return lists of file paths and labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvHXyWKcwaYI"
      },
      "source": [
        "## Step 8: Defining the Classifier Model\n",
        "We define a simple Multi-Layer Perceptron (MLP) model for classification. This model takes the embeddings from `Wav2Vec2` as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c8GgypE5whfJ"
      },
      "outputs": [],
      "source": [
        "class MLP(modlee.model.TabularClassificationModleeModel):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        # Define the model using nn.Sequential for simplicity\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 256),  # First fully connected layer\n",
        "            torch.nn.ReLU(),                   # ReLU activation\n",
        "            torch.nn.Linear(256, 128),          # Second fully connected layer\n",
        "            torch.nn.ReLU(),                   # ReLU activation\n",
        "            torch.nn.Linear(128, num_classes)   # Output layer\n",
        "        )\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the model\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y_target = batch\n",
        "        y_pred = self(x)\n",
        "        loss = self.loss_fn(y_pred, y_target) # Calculate the loss\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y_target = val_batch\n",
        "        y_pred = self(x)\n",
        "        val_loss = self.loss_fn(y_pred, y_target)  # Calculate validation loss\n",
        "        return {'val_loss': val_loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)  # Define the optimizer\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwQ3gGMkHAF"
      },
      "source": [
        "# Step 9: Precomputing Audio Embeddings Using `Wav2Vec2`\n",
        "`Wav2Vec2` transforms raw audio data into numerical embeddings that a model can interpret. We preprocess the audio by normalizing and padding it to a fixed length. Then, `Wav2Vec2` generates embeddings for each audio clip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pQzBrbJhkHIs"
      },
      "outputs": [],
      "source": [
        "def precompute_embeddings(dataloader):\n",
        "    embeddings_list = []\n",
        "    labels_list = []\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        embeddings = get_wav2vec_embeddings(inputs)  # Precompute embeddings\n",
        "        embeddings_list.append(embeddings.cpu())\n",
        "        labels_list.append(labels)\n",
        "    embeddings_list = torch.cat(embeddings_list, dim=0)  # Stack all embeddings\n",
        "    labels_list = torch.cat(labels_list, dim=0)  # Stack all labels\n",
        "    return embeddings_list, labels_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVdGOUaHwnET"
      },
      "source": [
        "## Step 10: Training and Evaluating the Model\n",
        "We create functions to train and evaluate our model. Training involves adjusting the model parameters to minimize the loss, while evaluation measures the model's performance on a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ni0k16XBwsrK"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, num_epochs=1):\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for embeddings, labels in dataloader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            outputs = model(embeddings)  # Get model predictions\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update model weights\n",
        "\n",
        "            running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for embeddings, labels in dataloader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(embeddings)  # Get model predictions\n",
        "            _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "            total += labels.size(0)  # Update total count\n",
        "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')  # Print accuracy percentage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQIxkcxwwGQ"
      },
      "source": [
        "## Step 11: Running the Main Script\n",
        "Finally, we load the dataset, preprocess it, and train the model.\n",
        "\n",
        "Add your path to the dataset in `data_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9jcHeLdwyBV",
        "outputId": "12f41231-576b-4944-eabd-43464ddbb645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputing embeddings for training and validation data...\n",
            "Epoch [1/1], Loss: 0.6355\n",
            "Accuracy: 82.79%\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Path to dataset\n",
        "    data_dir = '/Users/mansiagrawal/Downloads/Animals'  # Use the dataset containing 'cats', 'dogs', 'birds'\n",
        "\n",
        "    # Load dataset\n",
        "    audio_paths, labels = load_dataset(data_dir)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(audio_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    target_length = 16000  # Define the length for padding/truncation\n",
        "    train_dataset = AudioDataset(train_paths, train_labels, target_length=target_length)\n",
        "    val_dataset = AudioDataset(val_paths, val_labels, target_length=target_length)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "    # Precompute embeddings\n",
        "    print(\"Precomputing embeddings for training and validation data...\")\n",
        "    train_embeddings, train_labels = precompute_embeddings(train_dataloader)\n",
        "    val_embeddings, val_labels = precompute_embeddings(val_dataloader)\n",
        "\n",
        "    # Create TensorDataset for precomputed embeddings and labels\n",
        "    train_embedding_dataset = TensorDataset(train_embeddings, train_labels)\n",
        "    val_embedding_dataset = TensorDataset(val_embeddings, val_labels)\n",
        "\n",
        "    # Create DataLoaders for the precomputed embeddings\n",
        "    train_embedding_loader = DataLoader(train_embedding_dataset, batch_size=4, shuffle=True)\n",
        "    val_embedding_loader = DataLoader(val_embedding_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "    # Define number of classes\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    mlp_audio = MLP(input_size=768, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    train_model(mlp_audio, train_embedding_loader)\n",
        "    evaluate_model(mlp_audio, val_embedding_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I_T8JDdxG2J"
      },
      "source": [
        "# Amazing Work!\n",
        "\n",
        "We've successfully completed a machine learning project focused on audio classification using the `Wav2Vec2` model and a custom Multi-Layer Perceptron (MLP). Here's a quick recap of what we accomplished:\n",
        "\n",
        "- Loaded and prepared an audio dataset.\n",
        "- Extracted audio features using the `Wav2Vec2` model.\n",
        "- Built and trained a custom MLP for classification.\n",
        "- Evaluated the model's performance.\n",
        "\n",
        "This project has provided you with a solid understanding of how to leverage pre-trained models for feature extraction and integrate them with custom architectures for classification tasks. With this knowledge, you are well-equipped to experiment with other types of data, fine-tune model architectures, and continue advancing your machine learning skills. Keep exploring and building!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mansiagrawal/Documents/modlee_pypi/venv/lib/python3.12/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/var/folders/16/t5sssn69325c9k5wcws7c5fw0000gn/T/ipykernel_73368/2939050214.py:29: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")  # Set the audio backend to soundfile\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputing embeddings for training and validation data...\n",
            "Epoch [1/1], Loss: 0.7871\n",
            "Accuracy: 80.33%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import Wav2Vec2Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import modlee\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the pre-trained Wav2Vec2 model and move it to the specified device.\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
        "\n",
        "def get_wav2vec_embeddings(waveforms):\n",
        "    with torch.no_grad():  # Turn off gradients to save memory during inference\n",
        "        # Convert waveforms to a tensor and move it to the chosen device\n",
        "        inputs = torch.tensor(waveforms).to(device)\n",
        "        # Get embeddings from the Wav2Vec2 model\n",
        "        embeddings = wav2vec(inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "class AudioDataset(TensorDataset):\n",
        "    def __init__(self, audio_paths, labels, target_length=16000):\n",
        "        self.audio_paths = audio_paths  # List of paths to audio files\n",
        "        self.labels = labels  # List of labels corresponding to audio files\n",
        "        self.target_length = target_length  # Desired length for audio clips\n",
        "        torchaudio.set_audio_backend(\"soundfile\")  # Set the audio backend to soundfile\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_paths)  # Number of items in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_paths[idx]  # Get the path of the audio file\n",
        "        label = self.labels[idx]  # Get the label for the audio file\n",
        "        \n",
        "        # Load and normalize the audio, convert it to mono\n",
        "        waveform, sample_rate = torchaudio.load(audio_path, normalize=True)\n",
        "        waveform = waveform.mean(dim=0)  # Convert to mono by averaging channels\n",
        "\n",
        "        # Pad or truncate the waveform to the target length\n",
        "        if waveform.size(0) < self.target_length:\n",
        "            waveform = torch.cat([waveform, torch.zeros(self.target_length - waveform.size(0))])\n",
        "        else:\n",
        "            waveform = waveform[:self.target_length]\n",
        "\n",
        "        return waveform, label  # Return the processed waveform and its label\n",
        "\n",
        "def load_dataset(data_dir):\n",
        "    audio_paths = []  # List to store paths to audio files\n",
        "    labels = []  # List to store labels corresponding to each audio file\n",
        "\n",
        "    # Loop through each subdirectory in the data directory\n",
        "    for label_dir in os.listdir(data_dir):\n",
        "        label_dir_path = os.path.join(data_dir, label_dir)\n",
        "        if os.path.isdir(label_dir_path):  # Check if it's a directory\n",
        "            # Loop through each file in the directory\n",
        "            for file_name in os.listdir(label_dir_path):\n",
        "                if file_name.endswith('.wav'):  # Check if the file is a .wav file\n",
        "                    audio_paths.append(os.path.join(label_dir_path, file_name))  # Add file path to list\n",
        "                    labels.append(label_dir)  # Add label (directory name) to list\n",
        "\n",
        "    return audio_paths, labels  # Return lists of file paths and labels\n",
        "\n",
        "class MLP(modlee.model.TabularClassificationModleeModel):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        # Define the fully connected layers\n",
        "        self.fc1 = torch.nn.Linear(input_size, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, 128)\n",
        "        self.fc3 = torch.nn.Linear(128, num_classes)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y_target = batch\n",
        "        y_pred = self(x)\n",
        "        loss = self.loss_fn(y_pred, y_target)  # Calculate the loss\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y_target = val_batch\n",
        "        y_pred = self(x)\n",
        "        val_loss = self.loss_fn(y_pred, y_target)  # Calculate validation loss\n",
        "        return {'val_loss': val_loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)  # Define the optimizer\n",
        "        return optimizer\n",
        "\n",
        "def precompute_embeddings(dataloader):\n",
        "    embeddings_list = []\n",
        "    labels_list = []\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        embeddings = get_wav2vec_embeddings(inputs)  # Precompute embeddings\n",
        "        embeddings_list.append(embeddings.cpu())\n",
        "        labels_list.append(labels)\n",
        "    embeddings_list = torch.cat(embeddings_list, dim=0)  # Stack all embeddings\n",
        "    labels_list = torch.cat(labels_list, dim=0)  # Stack all labels\n",
        "    return embeddings_list, labels_list\n",
        "\n",
        "def train_model(model, dataloader, num_epochs=1):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for embeddings, labels in dataloader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            outputs = model(embeddings)  # Get model predictions\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "            optimizer.step()  # Update model weights\n",
        "\n",
        "            running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for embeddings, labels in dataloader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(embeddings)  # Get model predictions\n",
        "            _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
        "            total += labels.size(0)  # Update total count\n",
        "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "    accuracy = correct / total  # Compute accuracy\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')  # Print accuracy percentage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to dataset\n",
        "    data_dir = '/Users/mansiagrawal/Downloads/Animals'\n",
        "\n",
        "    # Load dataset\n",
        "    audio_paths, labels = load_dataset(data_dir)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(audio_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    target_length = 16000  # Define the length for padding/truncation\n",
        "    train_dataset = AudioDataset(train_paths, train_labels, target_length=target_length)\n",
        "    val_dataset = AudioDataset(val_paths, val_labels, target_length=target_length)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "    # Precompute embeddings\n",
        "    print(\"Precomputing embeddings for training and validation data...\")\n",
        "    train_embeddings, train_labels = precompute_embeddings(train_dataloader)\n",
        "    val_embeddings, val_labels = precompute_embeddings(val_dataloader)\n",
        "\n",
        "    # Create TensorDataset for precomputed embeddings and labels\n",
        "    train_embedding_dataset = TensorDataset(train_embeddings, train_labels)\n",
        "    val_embedding_dataset = TensorDataset(val_embeddings, val_labels)\n",
        "\n",
        "    # Create DataLoaders for the precomputed embeddings\n",
        "    train_embedding_loader = DataLoader(train_embedding_dataset, batch_size=4, shuffle=True)\n",
        "    val_embedding_loader = DataLoader(val_embedding_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "    # Define number of classes\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    mlp_audio = MLP(input_size=768, num_classes=num_classes).to(device)\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    train_model(mlp_audio, train_embedding_loader)\n",
        "    evaluate_model(mlp_audio, val_embedding_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08b37d0097fa4853812666fee8e13a43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1763c110771646e2a469cbf442c7b672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "246b015e6d2f4dda8d33769648149aba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bfa78db43e44802a478ad1fc8c796ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c80d636fccfb4283855af9724843334a",
            "max": 380267417,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca5c2d240740426882707a5045d12a1b",
            "value": 380267417
          }
        },
        "3f33159c1217480786ba4c275f4be9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c3c346b9654f93b107dba87fbd6b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a9fa9435dea424b912cfc451df59f9e",
              "IPY_MODEL_3bfa78db43e44802a478ad1fc8c796ee",
              "IPY_MODEL_6bb745aa8fcc4c968ca24eb537e8a5c1"
            ],
            "layout": "IPY_MODEL_08b37d0097fa4853812666fee8e13a43"
          }
        },
        "4a9fa9435dea424b912cfc451df59f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa191fed4144211af076236886f8dff",
            "placeholder": "​",
            "style": "IPY_MODEL_da64e8aa5619422e9b267faeae4f21cd",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "51109b3d3f1e4e81a3841696614d08f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae2205d65a148bc96f803393f95ae18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bb745aa8fcc4c968ca24eb537e8a5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa730613929e4934a283a29b344baeed",
            "placeholder": "​",
            "style": "IPY_MODEL_7c2744737c984e2b8eeea47e814aad35",
            "value": " 380M/380M [00:02&lt;00:00, 214MB/s]"
          }
        },
        "6d533c8d13414a38a27a5c398536404a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_246b015e6d2f4dda8d33769648149aba",
            "placeholder": "​",
            "style": "IPY_MODEL_bc067f3b8f22430e9c12a0b71e4951c6",
            "value": "config.json: 100%"
          }
        },
        "7c2744737c984e2b8eeea47e814aad35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83e47eac98f6488aaf72d7d7eb34f26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f33159c1217480786ba4c275f4be9c3",
            "placeholder": "​",
            "style": "IPY_MODEL_848e0109a6cb4a019336537873ec6fca",
            "value": " 1.84k/1.84k [00:00&lt;00:00, 143kB/s]"
          }
        },
        "848e0109a6cb4a019336537873ec6fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a887d9bdc6541c1a4274ee573bb8d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51109b3d3f1e4e81a3841696614d08f9",
            "max": 1842,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1763c110771646e2a469cbf442c7b672",
            "value": 1842
          }
        },
        "aa730613929e4934a283a29b344baeed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc067f3b8f22430e9c12a0b71e4951c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c80d636fccfb4283855af9724843334a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5c2d240740426882707a5045d12a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da64e8aa5619422e9b267faeae4f21cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daa191fed4144211af076236886f8dff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efaeb08642fd46a8a6b79a8842a3e7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d533c8d13414a38a27a5c398536404a",
              "IPY_MODEL_9a887d9bdc6541c1a4274ee573bb8d99",
              "IPY_MODEL_83e47eac98f6488aaf72d7d7eb34f26f"
            ],
            "layout": "IPY_MODEL_5ae2205d65a148bc96f803393f95ae18"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
