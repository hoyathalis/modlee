{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/mansiagr4/gifs/raw/main/new_small_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Text Embeddings With Tabular Classification Model\n",
    "\n",
    "This tutorial will guide you through a step-by-step breakdown of using a Multilayer Perceptron (MLP) with embeddings from a pre-trained `DistilBERT` model to classify text sentiment from the IMDB movie reviews dataset. We'll cover everything from dataset preprocessing to model evaluation, explaining each part in detail.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1oXsl5RtlZFJ6nFu6brGfn639YdYF6PCu?usp=sharing)\n",
    "\n",
    "In this section, we import the necessary libraries from `PyTorch` and `Torchvision`.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import modlee\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set our Modlee API key and initialize the Modlee package.\n",
    "Make sure that you have a Modlee account and an API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "# Set the API key to an environment variable,\n",
    "# to simulate setting this in your shell profile\n",
    "import os\n",
    "\n",
    "os.environ['MODLEE_API_KEY'] = \"replace-with-your-api-key\"\n",
    "modlee.init(api_key=os.environ['MODLEE_API_KEY'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP (Multilayer Perceptron) model is defined here as a neural network with three fully connected linear layers. Each layer is followed by a `ReLU` activation function.\n",
    "```python\n",
    "class MLP(modlee.model.TabularClassificationModleeModel):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),  # First fully connected layer\n",
    "            nn.ReLU(),                   # ReLU activation\n",
    "            nn.Linear(256, 128),          # Second fully connected layer\n",
    "            nn.ReLU(),                   # ReLU activation\n",
    "            nn.Linear(128, num_classes)   # Output layer\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the model defined in nn.Sequential\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_target = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y_target) # Calculate the loss\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y_target = val_batch\n",
    "        y_pred = self(x)\n",
    "        val_loss = self.loss_fn(y_pred, y_target)  # Calculate validation loss\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)  # Define the optimizer\n",
    "        return optimizer\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load `DistilBERT`, which is a more compact version of `BERT`. The tokenizer is responsible for converting raw text into a format that the `DistilBERT` model can understand.\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IMDB dataset using the Hugging Face datasets library. \n",
    "```python\n",
    "dataset = load_dataset('imdb')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since processing the entire dataset can be slow, we sample a subset of 1000 examples from the dataset to speed up computation.\n",
    "```python\n",
    "def sample_subset(dataset, subset_size=1000):\n",
    "    # Randomly shuffle dataset indices and select a subset\n",
    "    sample_indices = torch.randperm(len(dataset))[:subset_size]\n",
    "    # Select the sampled data based on the shuffled indices\n",
    "    sampled_data = dataset.select(sample_indices.tolist())\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DistilBERT` turns text into numerical embeddings that the model can understand. We first preprocess the text by tokenizing and padding it. Then, `DistilBERT` generates embeddings for each sentence.\n",
    "```python\n",
    "def get_text_embeddings(texts, tokenizer, bert, device, max_length=128):\n",
    "\n",
    "    # Tokenize the input texts, with padding and truncation to a fixed max length\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, \n",
    "                        max_length=max_length)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Get the embeddings from BERT without calculating gradients\n",
    "    with torch.no_grad():\n",
    "        # Average over the last hidden states to get sentence-level embeddings\n",
    "        embeddings = bert(input_ids, \n",
    "                        attention_mask=attention_mask).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Precompute embeddings for the entire dataset\n",
    "def precompute_embeddings(dataset, tokenizer, bert, device, max_length=128):\n",
    "    texts = dataset['text']  # Extract texts from the dataset\n",
    "    embeddings = get_text_embeddings(texts, tokenizer, bert, device, max_length)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `train_test_split` to split the precomputed embeddings and their corresponding labels into training and validation sets.\n",
    "```python\n",
    "def split_data(embeddings, labels):\n",
    "    # Split the embeddings and labels into training and validation sets \n",
    "    train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "        embeddings, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return train_embeddings, val_embeddings, train_labels, val_labels\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation data are batched using the `PyTorch DataLoader`, which ensures efficient processing during training.\n",
    "```python\n",
    "def create_dataloaders(train_embeddings, train_labels, val_embeddings, val_labels, batch_size):\n",
    "    # Create TensorDataset objects for training and validation data\n",
    "    train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "    val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "    # Create DataLoader objects to handle batching and shuffling of data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "    return train_loader, val_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_model` function defines the training loop. \n",
    "```python\n",
    "def train_model(model, train_loader, num_epochs=5):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer to update model weights\n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over batches of data\n",
    "        for embeddings, labels in train_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)  \n",
    "\n",
    "            # Forward pass: compute predictions and loss\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update model weights\n",
    "\n",
    "            running_loss += loss.item() * embeddings.size(0)  # Accumulate loss\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model is evaluated on the validation set. \n",
    "```python\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        # Iterate over validation data\n",
    "        for embeddings, labels in val_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device) \n",
    "            outputs = model(embeddings)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class labels\n",
    "\n",
    "            total += labels.size(0)  # Update total count\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the script, which follows these steps: loading and sampling the dataset, precomputing embeddings, training the MLP, and evaluating the model.\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Load and preprocess a subset of the IMDB dataset\n",
    "    train_data = sample_subset(dataset['train'], subset_size=1000)  \n",
    "    test_data = sample_subset(dataset['test'], subset_size=1000)  \n",
    "\n",
    "    # Precompute BERT embeddings to speed up training\n",
    "    print(\"Precomputing embeddings for training and testing data...\")\n",
    "    train_embeddings = precompute_embeddings(train_data, tokenizer, bert, device)  \n",
    "    test_embeddings = precompute_embeddings(test_data, tokenizer, bert, device) \n",
    "\n",
    "    # Convert labels from lists to tensors\n",
    "    train_labels = torch.tensor(train_data['label'], dtype=torch.long)\n",
    "    test_labels = torch.tensor(test_data['label'], dtype=torch.long)  \n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    train_embeddings, val_embeddings, train_labels, val_labels = split_data(train_embeddings, train_labels)  \n",
    "\n",
    "    # Create DataLoader instances for batching data\n",
    "    batch_size = 32  # Define batch size\n",
    "    train_loader, val_loader = create_dataloaders(train_embeddings, train_labels, val_embeddings, val_labels, batch_size)  \n",
    "\n",
    "    # Initialize and train the MLP model\n",
    "    input_size = 768  # Output size of BERT embeddings\n",
    "    num_classes = 2  \n",
    "    mlp_text = MLP(input_size=input_size, num_classes=num_classes).to(device) \n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(mlp_text, train_loader, num_epochs=5) \n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    print(\"Evaluating model...\")\n",
    "    evaluate_model(mlp_text, val_loader) \n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
