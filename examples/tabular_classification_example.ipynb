{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/mansiagr4/gifs/raw/main/new_small_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Classification\n",
    "\n",
    "This example notebook uses the `modlee` package to train a recommended model.\n",
    "We will perform image classification on CIFAR10 from `torchvision`.\n",
    "\n",
    "This examples uses the `modlee` package for tabular data classification. We'll use a diabetes dataset to show you how to:\n",
    "\n",
    "1. Prepare the data.\n",
    "2. Use `modlee` for model training.\n",
    "3. Implement and train a custom model.\n",
    "4. Evaluate the model.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rf9BOCFADV2BtyY6xCDIu2JGz-na96t1?authuser=1#scrollTo=Qx9LuWnomG_5)\n",
    "\n",
    "First, we will import the the necessary libraries and set up the environment. \n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import os\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "```\n",
    "Now, we will set up the `modlee` API key and initialize the `modlee` package. You can access your `modlee` API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "os.environ['MODLEE_API_KEY'] = \"replace-with-your-api-key\"\n",
    "modlee.init(api_key=os.environ['MODLEE_API_KEY'])\n",
    "```\n",
    "Now, we will prepare our data. For this example, we will manually download the diabetes dataset from Kaggle and upload it to the environment.\n",
    "\n",
    "Visit the [Diabetes CSV dataset page](https://www.kaggle.com/datasets/saurabh00007/diabetescsv) on Kaggle and click the **Download** button to save the dataset `diabetes.csv` to your local machine. \n",
    "\n",
    "Copy the path to that donwloaded file, which will be used later. \n",
    "Define a custom dataset class `TabularDataset` for handling our tabular data.\n",
    "```python\n",
    "class TabularDataset(TensorDataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)  # Convert features to tensors\n",
    "        self.target = torch.tensor(target, dtype=torch.long) # Convert labels to long integers for classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) # Return the size of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx] # Return a single sample from the dataset\n",
    "```\n",
    "We can now load and preprocess the data, and also create the dataloaders. \n",
    "```python\n",
    "\n",
    "def get_diabetes_dataloaders(batch_size=32, val_split=0.2, shuffle=True):\n",
    "    dataset_path = \"/Users/mansiagrawal/Downloads/diabetes.csv\"\n",
    "    df = pd.read_csv(dataset_path) # Load the CSV file into a DataFrame\n",
    "    X = df.drop('Outcome', axis=1).values # Features (X) - drop the target column\n",
    "    y = df['Outcome'].values # Labels (y) - the target column\n",
    "    scaler = StandardScaler() # Initialize the scaler for feature scaling\n",
    "    X_scaled = scaler.fit_transform(X) # Scale the features\n",
    "    dataset = TabularDataset(X_scaled, y) # Create a TabularDataset instance\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    dataset_size = len(dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoader instances for training and validation\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Generate the DataLoaders\n",
    "train_dataloader, val_dataloader = get_diabetes_dataloaders(batch_size=32, val_split=0.2, shuffle=True)\n",
    "```\n",
    "Next, we will define our custom model, which is a simple feedforward neural network called `TabularClassifier`. This model will be integtated with Modlee's framework.\n",
    "\n",
    "```python\n",
    "class TabularClassifier(modlee.model.TabularClassificationModleeModel):\n",
    "    def __init__(self, input_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 128)  # First hidden layer\n",
    "        self.dropout1 = torch.nn.AlphaDropout(0.1)  # Dropout to prevent overfitting\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(128, 64)  # Second hidden layer\n",
    "        self.dropout2 = torch.nn.AlphaDropout(0.1)  # Dropout to prevent overfitting\n",
    "\n",
    "        self.fc3 = torch.nn.Linear(64, 32)  # Third hidden layer\n",
    "        self.dropout3 = torch.nn.AlphaDropout(0.1)  # Dropout to prevent overfitting\n",
    "\n",
    "        self.fc4 = torch.nn.Linear(32, num_classes)  # Output layer\n",
    "\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.selu(self.fc1(x))  # Apply SELU activation to the first layer\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "\n",
    "        x = torch.selu(self.fc2(x))  # Apply SELU activation to the second layer\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "\n",
    "        x = torch.selu(self.fc3(x))  # Apply SELU activation to the third layer\n",
    "        x = self.dropout3(x)  # Apply dropout\n",
    "\n",
    "        x = self.fc4(x)  # Output layer without activation (for binary classification)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_target = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y_target.squeeze()) # Calculate the loss\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y_target = val_batch\n",
    "        y_pred = self(x)\n",
    "        val_loss = self.loss_fn(y_pred, y_target.squeeze()) # Calculate validation loss\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)  # Define the optimizer\n",
    "        return optimizer\n",
    "```\n",
    "Next, we can train our model using `PyTorch Lightning` for one epoch.\n",
    "```python\n",
    "# Get the input dimension\n",
    "original_train_dataset = train_dataloader.dataset.dataset # Access the original dataset\n",
    "input_dim = len(original_train_dataset[0][0])\n",
    "num_classes = 2  # Binary classification\n",
    "\n",
    "# Initialize the Modlee model\n",
    "modlee_model = TabularClassifier(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# Train the model using PyTorch Lightning\n",
    "with modlee.start_run() as run:\n",
    "    trainer = pl.Trainer(max_epochs=1)\n",
    "    trainer.fit(\n",
    "        model=modlee_model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "\n",
    "```\n",
    "We will evaluate the model now by predicting on the validation set and calculating the accuracy.\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate the model's performance\n",
    "modlee_model.eval() # Set the model to evaluation mode\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad(): # Disable gradient computation\n",
    "    for batch in val_dataloader:\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = modlee_model(X_batch) # Get model predictions\n",
    "        predictions = torch.argmax(outputs, dim=1)  # Round predictions to get binary output\n",
    "        y_pred.extend(predictions.numpy()) # Store predictions\n",
    "        y_true.extend(y_batch.numpy()) # Store true labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "After training, we inspect the artifacts saved by Modlee, including the model graph and various statistics.\n",
    "```python\n",
    "import sys\n",
    "\n",
    "# Get the path to the last run's saved data\n",
    "last_run_path = modlee.last_run_path()\n",
    "print(f\"Run path: {last_run_path}\")\n",
    "\n",
    "# Get the path to the saved artifacts\n",
    "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
    "artifacts = os.listdir(artifacts_path)\n",
    "print(f\"Saved artifacts: {artifacts}\")\n",
    "\n",
    "# Set the artifacts path as an environment variable\n",
    "os.environ['ARTIFACTS_PATH'] = artifacts_path\n",
    "\n",
    "# Add the artifacts directory to the system path\n",
    "sys.path.insert(0, artifacts_path)\n",
    "```\n",
    "```python\n",
    "# Print out the first few lines of the model\n",
    "print(\"Model graph:\")\n",
    "```\n",
    "\n",
    "```shell\n",
    "!sed -n -e 1,15p $ARTIFACTS_PATH/model_graph.py\n",
    "!echo \"        ...\"\n",
    "!sed -n -e 58,68p $ARTIFACTS_PATH/model_graph.py\n",
    "!echo \"        ...\"\n",
    "```\n",
    "```python\n",
    "# Print the first lines of the data metafeatures\n",
    "print(\"Data metafeatures:\")\n",
    "```\n",
    "\n",
    "```shell\n",
    "!head -20 $ARTIFACTS_PATH/stats_rep\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
