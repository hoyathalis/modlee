{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/mansiagr4/gifs/raw/main/new_small_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Audio Embeddings With Tabular Classification Model\n",
    "\n",
    "In this example, we will build an audio classification model using `PyTorch` and `Wav2Vec2`, a pretrained model for processing audio data. This guide will walk you through each step of the process, including setting up the environment, loading and preprocessing data, defining and training a model, and evaluating its performance.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tjrD_tUB7tbQuR6kJ_mPM-_kCbVY-Q71?usp=sharing#scrollTo=Ys9Rj0sVqrl8)\n",
    "\n",
    "First, we will import the the necessary libraries and set up the environment. \n",
    "```python\n",
    "import torchaudio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import Wav2Vec2Model\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import os\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "```\n",
    "Now we will set our Modlee API key and initialize the Modlee package.\n",
    "Make sure that you have a Modlee account and an API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "# Set the API key to an environment variable,\n",
    "# to simulate setting this in your shell profile\n",
    "os.environ['MODLEE_API_KEY'] = \"replace-with-your-api-key\"\n",
    "modlee.init(api_key=os.environ['MODLEE_API_KEY'])\n",
    "\n",
    "```\n",
    "Now, we will prepare our data. For this example, we will manually download the `Human Words` dataset from Kaggle and upload it to the environment.\n",
    "\n",
    "Visit the [Human Words Audio dataset page](https://www.kaggle.com/datasets/warcoder/cats-vs-dogs-vs-birds-audio-classification?resource=download) on Kaggle and click the **Download** button to save the `Animals` directory to your local machine. \n",
    "\n",
    "Copy the path to that donwloaded file, which will be used later. \n",
    "This snippet loads the `Wav2Vec2 model`. We'll use it to convert audio into embeddings.\n",
    "\n",
    "\n",
    "This snippet loads the `Wav2Vec2` model. `Wav2Vec2` is a model designed for speech processing. We'll use it to convert audio into embeddings.\n",
    "\n",
    "```python\n",
    "# Set device to GPU if available, otherwise use CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained Wav2Vec2 model and move it to the specified device.\n",
    "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "```\n",
    "This function converts raw audio waveforms into embeddings using the `Wav2Vec2` model.\n",
    "```python\n",
    "def get_wav2vec_embeddings(waveforms):\n",
    "    with torch.no_grad():  # Turn off gradients to save memory during inference\n",
    "        # Convert waveforms to a tensor and move it to the chosen device\n",
    "        inputs = torch.tensor(waveforms).to(device)\n",
    "        # Get embeddings from the Wav2Vec2 model\n",
    "        embeddings = wav2vec(inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "```\n",
    "The `AudioDataset` class handles loading and preprocessing of audio files. \n",
    "```python\n",
    "class AudioDataset(TensorDataset):\n",
    "    def __init__(self, audio_paths, labels, target_length=16000):\n",
    "        self.audio_paths = audio_paths  # List of paths to audio files\n",
    "        self.labels = labels  # List of labels corresponding to audio files\n",
    "        self.target_length = target_length  # Desired length for audio clips\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)  # Number of items in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]  # Get the path of the audio file\n",
    "        label = self.labels[idx]  # Get the label for the audio file\n",
    "        waveform, sample_rate = torchaudio.load(audio_path, normalize=True)  \n",
    "        waveform = waveform.mean(dim=0)  # Convert to mono by averaging channels\n",
    "\n",
    "        # Pad or truncate the waveform to the target length\n",
    "        if waveform.size(0) < self.target_length:\n",
    "            waveform = torch.cat([waveform, torch.zeros(self.target_length - waveform.size(0))])\n",
    "        else:\n",
    "            waveform = waveform[:self.target_length]\n",
    "\n",
    "        return waveform, label  \n",
    "\n",
    "```\n",
    "This function loads audio files and their corresponding labels from a directory structure.\n",
    "```python\n",
    "def load_dataset(data_dir):\n",
    "    audio_paths = []  # List to store paths to audio files\n",
    "    labels = []  # List to store labels corresponding to each audio file\n",
    "\n",
    "    # Loop through each subdirectory in the data directory\n",
    "    for label_dir in os.listdir(data_dir):\n",
    "        label_dir_path = os.path.join(data_dir, label_dir)\n",
    "        if os.path.isdir(label_dir_path):  # Check if it's a directory\n",
    "            # Loop through each file in the directory\n",
    "            for file_name in os.listdir(label_dir_path):\n",
    "                if file_name.endswith('.wav'):  # Check if the file is a .wav file\n",
    "                    audio_paths.append(os.path.join(label_dir_path, file_name))  \n",
    "                    labels.append(label_dir)  # Add label (directory name) to list\n",
    "\n",
    "    return audio_paths, labels  # Return lists of file paths and labels\n",
    "\n",
    "```\n",
    "We define a simple Multi-Layer Perceptron (MLP) model for classification. This model takes the embeddings from `Wav2Vec2` as input.\n",
    "```python\n",
    "class MLP(modlee.model.TabularClassificationModleeModel):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        # Define the model using nn.Sequential for simplicity\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 256),  # First fully connected layer\n",
    "            torch.nn.ReLU(),                   # ReLU activation\n",
    "            torch.nn.Linear(256, 128),          # Second fully connected layer\n",
    "            torch.nn.ReLU(),                   # ReLU activation\n",
    "            torch.nn.Linear(128, num_classes)   # Output layer\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the model\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_target = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y_target) # Calculate the loss\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y_target = val_batch\n",
    "        y_pred = self(x)\n",
    "        val_loss = self.loss_fn(y_pred, y_target)  # Calculate validation loss\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)  # Define the optimizer\n",
    "        return optimizer\n",
    "```\n",
    "\n",
    "`Wav2Vec2` transforms raw audio data into numerical embeddings that a model can interpret. We preprocess the audio by normalizing and padding it to a fixed length. Then, `Wav2Vec2` generates embeddings for each audio clip.\n",
    "```python\n",
    "def precompute_embeddings(dataloader):\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        embeddings = get_wav2vec_embeddings(inputs)  # Precompute embeddings\n",
    "        embeddings_list.append(embeddings.cpu())\n",
    "        labels_list.append(labels)\n",
    "    embeddings_list = torch.cat(embeddings_list, dim=0)  # Stack all embeddings\n",
    "    labels_list = torch.cat(labels_list, dim=0)  # Stack all labels\n",
    "    return embeddings_list, labels_list\n",
    "```\n",
    "We create functions to train and evaluate our model.\n",
    "```python\n",
    "def train_model(model, dataloader, num_epochs=1):\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for embeddings, labels in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(embeddings)  # Get model predictions\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update model weights\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "```\n",
    "\n",
    "```python\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for embeddings, labels in dataloader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(embeddings)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class labels\n",
    "            total += labels.size(0)  # Update total count\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = correct / total  # Compute accuracy\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')  # Print accuracy percentage\n",
    "```\n",
    "Finally, we load the dataset, preprocess it, and train the model.\n",
    "\n",
    "Add your path to the dataset in `data_dir`.\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to dataset\n",
    "    data_dir = 'path-to-dataset'  # Use the dataset containing 'cats', 'dogs', 'birds'\n",
    "\n",
    "    # Load dataset\n",
    "    audio_paths, labels = load_dataset(data_dir)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(audio_paths, labels, \n",
    "                                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    target_length = 16000  # Define the length for padding/truncation\n",
    "    train_dataset = AudioDataset(train_paths, train_labels, target_length=target_length)\n",
    "    val_dataset = AudioDataset(val_paths, val_labels, target_length=target_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # Precompute embeddings\n",
    "    print(\"Precomputing embeddings for training and validation data...\")\n",
    "    train_embeddings, train_labels = precompute_embeddings(train_dataloader)\n",
    "    val_embeddings, val_labels = precompute_embeddings(val_dataloader)\n",
    "\n",
    "    # Create TensorDataset for precomputed embeddings and labels\n",
    "    train_embedding_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "    val_embedding_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "    # Create DataLoaders for the precomputed embeddings\n",
    "    train_embedding_loader = DataLoader(train_embedding_dataset, batch_size=4, shuffle=True)\n",
    "    val_embedding_loader = DataLoader(val_embedding_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # Define number of classes\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    mlp_audio = MLP(input_size=768, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    train_model(mlp_audio, train_embedding_loader)\n",
    "    evaluate_model(mlp_audio, val_embedding_loader)\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
