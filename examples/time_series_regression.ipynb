{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/mansiagr4/gifs/raw/main/new_small_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Regression\n",
    "\n",
    "In this tutorial, we will guide you through the process of implementing a time series regression model using the Modlee framework along with `PyTorch`. \n",
    "\n",
    "The goal is to predict power consumption based on various environmental factors, such as temperature, humidity, wind speed, and solar radiation. \n",
    "\n",
    "**Note**: Currently, Modlee does not support recurrent LSTM operations. Instead, we will focus on non-recurrent models suited for time series data, such as convolutional neural networks (CNNs) and transformers, which can effectively capture sequential patterns without requiring recurrent layers.\n",
    "\n",
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/modlee/time-series-regression)\n",
    "\n",
    "First, we will import the the necessary libraries and set up the environment. \n",
    "```python\n",
    "import torch\n",
    "import os\n",
    "import modlee\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "Now, we will set up the `modlee` API key and initialize the `modlee` package. You can access your `modlee` API key [from the dashboard](https://www.dashboard.modlee.ai/).\n",
    "\n",
    "Replace `replace-with-your-api-key` with your API key.\n",
    "```python\n",
    "modlee.init(api_key=\"replace-with-your-api-key\")\n",
    "```\n",
    "\n",
    "The dataset used in this tutorial includes hourly time series data that links environmental conditions to power consumption across three zones. Each record contains a timestamp, temperature, humidity, wind speed, and measures of solar radiation, alongside the power consumption (in watts) for each zone. \n",
    "\n",
    "This data allows for the exploration of relationships between weather patterns and energy usage, aiding in the development of predictive models. \n",
    "\n",
    "For this example, we will manually download the dataset from Kaggle and upload it to the environment. Visit the [Time Series Regression dataset page](https://www.kaggle.com/datasets/modlee/time-series-regression-data) on Kaggle and click the **Download** button to save the dataset to your local machine. \n",
    "\n",
    "Copy the path to the donwloaded files, which will be used later. \n",
    "\n",
    "Next, we need to load the power consumption dataset. This dataset contains various features related to environmental conditions and their corresponding power consumption values. The `load_power_consumption_data` function is designed to read the CSV file, process the data, and create time series sequences.\n",
    "\n",
    "We then select the relevant features from the dataset for our input variables, `X`, which include temperature, humidity, wind speed, and solar radiation values. The output variable, `y`, is calculated as the mean power consumption across three different zones.\n",
    "\n",
    "```python\n",
    "# Function to load the power consumption dataset and prepare it for training\n",
    "def load_power_consumption_data(file_path, seq_length):\n",
    "    # Load the dataset from the specified CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Convert the 'Datetime' column to datetime objects\n",
    "    data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "    # Set the 'Datetime' column as the index for the DataFrame\n",
    "    data.set_index('Datetime', inplace=True)\n",
    "    \n",
    "    # Extract relevant features for prediction and target variable\n",
    "    X = data[['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows', 'DiffuseFlows']].values\n",
    "    # Calculate the average power consumption across the three zones as the target variable\n",
    "    y = data[['PowerConsumption_Zone1', 'PowerConsumption_Zone2', 'PowerConsumption_Zone3']].mean(axis=1).values  \n",
    "    # Convert features and target to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Create sequences of the specified length for input features\n",
    "    num_samples = X.shape[0] - seq_length + 1\n",
    "    X_seq = torch.stack([X[i:i + seq_length] for i in range(num_samples)])\n",
    "    y_seq = y[seq_length - 1:]  # Align target variable with sequences\n",
    "\n",
    "    return X_seq, y_seq\n",
    "```\n",
    "\n",
    "Once we have the preprocessed data, we proceed to create `PyTorch` datasets and `DataLoaders`. \n",
    "\n",
    "Here, we load the power consumption data from the specified CSV file. We create a `TensorDataset` to hold the features and labels. To split the dataset into training and validation sets, we use the `train_test_split` function from `sklearn`. \n",
    "\n",
    "```python\n",
    "# Define the path to the dataset\n",
    "file_path = 'path-to-powerconsumption.csv'\n",
    "# Load the power consumption data with a specified sequence length\n",
    "X, y = load_power_consumption_data(file_path, 20)\n",
    "\n",
    "# Create a TensorDataset for the training data\n",
    "dataset = TensorDataset(X, y)\n",
    "# Split dataset indices into training and validation sets\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = TensorDataset(X[train_indices], y[train_indices])\n",
    "val_dataset = TensorDataset(X[val_indices], y[val_indices])\n",
    "\n",
    "# Create DataLoader for batch processing during training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "We will now define a multivariate time series regression model by creating a class that inherits from `modlee.model.TimeseriesRegressionModleeModel`. This class uses a Transformer-based architecture to predict a continuous value.\n",
    "\n",
    "We initialize a `TransformerEncoder` with multi-head attention to process sequential dependencies.\n",
    "\n",
    "```python\n",
    "class TransformerTimeSeriesRegressor(modlee.model.TimeseriesRegressionModleeModel):\n",
    "    def __init__(self, input_dim, seq_length, num_heads=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Initialize a Transformer encoder layer with specified input dimensions and heads\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n",
    "        # Stack encoder layers to form the Transformer encoder\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        # Define a fully connected layer to map encoded features to a single output value\n",
    "        self.fc = torch.nn.Linear(input_dim * seq_length, 1)\n",
    "        # Set the loss function to mean squared error for regression tasks\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        # Flatten the output and pass it through the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # Get input and target from batch\n",
    "        x, y = batch\n",
    "        # Generate predictions and compute loss\n",
    "        preds = self.forward(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        # Get input and target from batch\n",
    "        x, y = batch\n",
    "        # Generate predictions and compute loss\n",
    "        preds = self.forward(x)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use the Adam optimizer with a learning rate of 1e-3\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "model = TransformerTimeSeriesRegressor(input_dim=5, seq_length=20)\n",
    "```\n",
    "\n",
    "With our model defined, we can now train it using the `PyTorch Lightning Trainer`. This trainer simplifies the training process by managing the training loops and logging.\n",
    "\n",
    "```python\n",
    "# Start a training run with Modlee\n",
    "with modlee.start_run() as run:\n",
    "    trainer = pl.Trainer(max_epochs=1)  # Set up the trainer\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        train_dataloaders=train_dataloader,  # Load training data\n",
    "        val_dataloaders=val_dataloader  # Load validation data\n",
    "    )\n",
    "```\n",
    "\n",
    "After training, we inspect the artifacts saved by Modlee, including the model graph and various statistics. With Modlee, your training assets are automatically saved, preserving valuable insights for future reference and collaboration.\n",
    "\n",
    "```python\n",
    "last_run_path = modlee.last_run_path()\n",
    "print(f\"Run path: {last_run_path}\")\n",
    "artifacts_path = os.path.join(last_run_path, 'artifacts')\n",
    "artifacts = sorted(os.listdir(artifacts_path))\n",
    "print(f\"Saved artifacts: {artifacts}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
